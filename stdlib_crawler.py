#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— ä¾èµ–ç½‘ç»œçˆ¬è™« - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“
No-Dependency Web Crawler - Using Only Python Standard Library
"""

import urllib.request
import urllib.parse
import urllib.error
from html.parser import HTMLParser
import json
import os
import time
import re
from collections import defaultdict
from datetime import datetime


class HTMLDataExtractor(HTMLParser):
    """HTMLæ•°æ®æå–å™¨ - ä½¿ç”¨æ ‡å‡†åº“çš„HTMLParser"""

    def __init__(self):
        super().__init__()
        self.title = ""
        self.links = []
        self.images = []
        self.text_content = []
        self.meta_data = {}

        # çŠ¶æ€è·Ÿè¸ª
        self._in_title = False
        self._in_script = False
        self._in_style = False
        self._current_tag = None

    def handle_starttag(self, tag, attrs):
        """å¤„ç†å¼€å§‹æ ‡ç­¾"""
        self._current_tag = tag
        attrs_dict = dict(attrs)

        # æå–æ ‡é¢˜
        if tag == 'title':
            self._in_title = True

        # æå–é“¾æ¥
        elif tag == 'a' and 'href' in attrs_dict:
            self.links.append({
                'url': attrs_dict['href'],
                'text': ''
            })

        # æå–å›¾ç‰‡
        elif tag == 'img':
            img_data = {
                'src': attrs_dict.get('src', ''),
                'alt': attrs_dict.get('alt', ''),
                'title': attrs_dict.get('title', '')
            }
            self.images.append(img_data)

        # æå–metaä¿¡æ¯
        elif tag == 'meta':
            name = attrs_dict.get('name', attrs_dict.get('property', ''))
            content = attrs_dict.get('content', '')
            if name and content:
                self.meta_data[name] = content

        # è·Ÿè¸ªscriptå’Œstyleæ ‡ç­¾
        elif tag == 'script':
            self._in_script = True
        elif tag == 'style':
            self._in_style = True

    def handle_endtag(self, tag):
        """å¤„ç†ç»“æŸæ ‡ç­¾"""
        if tag == 'title':
            self._in_title = False
        elif tag == 'script':
            self._in_script = False
        elif tag == 'style':
            self._in_style = False

    def handle_data(self, data):
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        # æå–æ ‡é¢˜
        if self._in_title:
            self.title += data.strip()

        # æå–é“¾æ¥æ–‡æœ¬
        if self._current_tag == 'a' and self.links:
            self.links[-1]['text'] += data.strip()

        # æå–æ­£æ–‡å†…å®¹ï¼ˆè·³è¿‡scriptå’Œstyleï¼‰
        if not self._in_script and not self._in_style:
            text = data.strip()
            if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                self.text_content.append(text)


class StdlibCrawler:
    """åªä½¿ç”¨æ ‡å‡†åº“çš„ç½‘ç»œçˆ¬è™«"""

    def __init__(self, base_url, max_pages=10, delay=1, download_images=False):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            base_url: èµ·å§‹URL
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.download_images = download_images

        self.visited_urls = set()
        self.to_visit = [base_url]
        self.pages_data = []
        self.stats = defaultdict(int)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'crawler_output'
        self.images_dir = os.path.join(self.output_dir, 'images')
        self.pages_dir = os.path.join(self.output_dir, 'pages')

        for directory in [self.output_dir, self.images_dir, self.pages_dir]:
            os.makedirs(directory, exist_ok=True)

    def is_valid_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å±äºåŒä¸€åŸŸå"""
        try:
            parsed_base = urllib.parse.urlparse(self.base_url)
            parsed_url = urllib.parse.urlparse(url)

            # åªçˆ¬å–åŒä¸€åŸŸåä¸‹çš„é¡µé¢
            return parsed_url.netloc == parsed_base.netloc
        except:
            return False

    def normalize_url(self, url, base_url):
        """æ ‡å‡†åŒ–URL"""
        # ç§»é™¤fragment
        url = urllib.parse.urljoin(base_url, url)
        parsed = urllib.parse.urlparse(url)

        # é‡å»ºURLï¼Œä¸åŒ…å«fragment
        return urllib.parse.urlunparse(
            (parsed.scheme, parsed.netloc, parsed.path,
             parsed.params, parsed.query, '')
        )

    def fetch_page(self, url):
        """
        è·å–ç½‘é¡µå†…å®¹

        Args:
            url: è¦è·å–çš„URL

        Returns:
            ç½‘é¡µå†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            request = urllib.request.Request(url, headers=headers)

            with urllib.request.urlopen(request, timeout=10) as response:
                # è·å–ç¼–ç 
                content_type = response.headers.get('Content-Type', '')
                encoding = 'utf-8'

                # å°è¯•ä»Content-Typeæå–ç¼–ç 
                charset_match = re.search(r'charset=([^\s;]+)', content_type)
                if charset_match:
                    encoding = charset_match.group(1)

                # è¯»å–å†…å®¹
                content = response.read()

                try:
                    return content.decode(encoding)
                except:
                    # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                    for enc in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                        try:
                            return content.decode(enc)
                        except:
                            continue
                    return content.decode('utf-8', errors='ignore')

        except urllib.error.HTTPError as e:
            print(f"   âŒ HTTPé”™è¯¯ {e.code}: {url}")
            self.stats['http_errors'] += 1
            return None
        except urllib.error.URLError as e:
            print(f"   âŒ URLé”™è¯¯: {url} - {e.reason}")
            self.stats['url_errors'] += 1
            return None
        except Exception as e:
            print(f"   âŒ æœªçŸ¥é”™è¯¯: {url} - {e}")
            self.stats['other_errors'] += 1
            return None

    def download_image(self, img_url, page_url, index):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # æ ‡å‡†åŒ–å›¾ç‰‡URL
            full_url = urllib.parse.urljoin(page_url, img_url)

            # ç”Ÿæˆæ–‡ä»¶å
            parsed = urllib.parse.urlparse(full_url)
            ext = os.path.splitext(parsed.path)[1] or '.jpg'
            filename = f"img_{int(time.time())}_{index}{ext}"
            filepath = os.path.join(self.images_dir, filename)

            # ä¸‹è½½å›¾ç‰‡
            request = urllib.request.Request(
                full_url,
                headers={'User-Agent': 'Mozilla/5.0'}
            )

            with urllib.request.urlopen(request, timeout=10) as response:
                with open(filepath, 'wb') as f:
                    f.write(response.read())

            self.stats['images_downloaded'] += 1
            return filename
        except Exception as e:
            print(f"   âš ï¸  å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url} - {e}")
            return None

    def save_page_content(self, url, html_content, page_num):
        """ä¿å­˜ç½‘é¡µå†…å®¹åˆ°æ–‡ä»¶"""
        try:
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            filename = f"page_{page_num:03d}.html"
            filepath = os.path.join(self.pages_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"<!-- URL: {url} -->\n")
                f.write(f"<!-- Downloaded: {datetime.now()} -->\n\n")
                f.write(html_content)

            return filename
        except Exception as e:
            print(f"   âš ï¸  ä¿å­˜é¡µé¢å¤±è´¥: {e}")
            return None

    def parse_html(self, html_content, page_url):
        """è§£æHTMLå†…å®¹"""
        parser = HTMLDataExtractor()

        try:
            parser.feed(html_content)
        except Exception as e:
            print(f"   âš ï¸  HTMLè§£æè­¦å‘Š: {e}")

        # æ ‡å‡†åŒ–æ‰€æœ‰é“¾æ¥
        normalized_links = []
        for link in parser.links:
            try:
                normalized_url = self.normalize_url(link['url'], page_url)
                normalized_links.append({
                    'url': normalized_url,
                    'text': link['text']
                })
            except:
                pass

        return {
            'title': parser.title or 'æ— æ ‡é¢˜',
            'links': normalized_links,
            'images': parser.images,
            'text_content': parser.text_content,
            'meta_data': parser.meta_data
        }

    def crawl(self):
        """å¼€å§‹çˆ¬å–"""
        print("\n" + "=" * 80)
        print("ğŸš€ æ ‡å‡†åº“ç½‘ç»œçˆ¬è™«å¯åŠ¨")
        print("=" * 80)
        print(f"ğŸ“ èµ·å§‹URL: {self.base_url}")
        print(f"ğŸ“„ æœ€å¤§é¡µé¢: {self.max_pages}")
        print(f"â±ï¸  è¯·æ±‚å»¶è¿Ÿ: {self.delay}ç§’")
        print(f"ğŸ–¼ï¸  ä¸‹è½½å›¾ç‰‡: {'æ˜¯' if self.download_images else 'å¦'}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}/")
        print("=" * 80 + "\n")

        page_count = 0
        start_time = time.time()

        while self.to_visit and page_count < self.max_pages:
            # è·å–ä¸‹ä¸€ä¸ªURL
            current_url = self.to_visit.pop(0)

            # è·³è¿‡å·²è®¿é—®çš„URL
            if current_url in self.visited_urls:
                continue

            print(f"ğŸ“„ [{page_count + 1}/{self.max_pages}] {current_url}")

            # æ ‡è®°ä¸ºå·²è®¿é—®
            self.visited_urls.add(current_url)

            # è·å–é¡µé¢
            html_content = self.fetch_page(current_url)

            if html_content:
                # ä¿å­˜åŸå§‹HTML
                saved_file = self.save_page_content(current_url, html_content, page_count + 1)

                # è§£æHTML
                parsed_data = self.parse_html(html_content, current_url)

                print(f"   âœ… {parsed_data['title']}")
                print(f"   ğŸ“Š é“¾æ¥: {len(parsed_data['links'])}, "
                      f"å›¾ç‰‡: {len(parsed_data['images'])}, "
                      f"æ–‡æœ¬æ®µ: {len(parsed_data['text_content'])}")

                # ä¸‹è½½å›¾ç‰‡
                if self.download_images and parsed_data['images']:
                    downloaded = []
                    for idx, img in enumerate(parsed_data['images'][:5]):  # æœ€å¤š5å¼ 
                        filename = self.download_image(img['src'], current_url, idx)
                        if filename:
                            downloaded.append(filename)
                    if downloaded:
                        print(f"   ğŸ–¼ï¸  å·²ä¸‹è½½ {len(downloaded)} å¼ å›¾ç‰‡")

                # ä¿å­˜æ•°æ®
                page_data = {
                    'url': current_url,
                    'title': parsed_data['title'],
                    'saved_file': saved_file,
                    'links_count': len(parsed_data['links']),
                    'images_count': len(parsed_data['images']),
                    'text_segments': len(parsed_data['text_content']),
                    'meta_data': parsed_data['meta_data']
                }
                self.pages_data.append(page_data)

                # æ›´æ–°ç»Ÿè®¡
                self.stats['pages_crawled'] += 1
                self.stats['total_links'] += len(parsed_data['links'])
                self.stats['total_images'] += len(parsed_data['images'])

                # æ·»åŠ æ–°é“¾æ¥åˆ°å¾…è®¿é—®åˆ—è¡¨
                for link in parsed_data['links']:
                    link_url = link['url']
                    if (self.is_valid_url(link_url) and
                        link_url not in self.visited_urls and
                        link_url not in self.to_visit):
                        self.to_visit.append(link_url)

                page_count += 1

            # å»¶è¿Ÿ
            if page_count < self.max_pages and self.to_visit:
                time.sleep(self.delay)

        # è®¡ç®—æ€»æ—¶é—´
        elapsed_time = time.time() - start_time

        # ä¿å­˜ç»“æœ
        self.save_results(elapsed_time)

        # æ˜¾ç¤ºç»Ÿè®¡
        self.print_statistics(elapsed_time)

    def save_results(self, elapsed_time):
        """ä¿å­˜çˆ¬å–ç»“æœä¸ºJSON"""
        results = {
            'base_url': self.base_url,
            'crawl_time': datetime.now().isoformat(),
            'elapsed_seconds': round(elapsed_time, 2),
            'statistics': dict(self.stats),
            'pages': self.pages_data
        }

        results_file = os.path.join(self.output_dir, 'results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def print_statistics(self, elapsed_time):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        print("=" * 80)
        print(f"âœ… æˆåŠŸçˆ¬å–: {self.stats['pages_crawled']} ä¸ªé¡µé¢")
        print(f"ğŸ”— å‘ç°é“¾æ¥: {self.stats['total_links']} ä¸ª")
        print(f"ğŸ–¼ï¸  å‘ç°å›¾ç‰‡: {self.stats['total_images']} å¼ ")

        if self.download_images:
            print(f"ğŸ“¥ ä¸‹è½½å›¾ç‰‡: {self.stats['images_downloaded']} å¼ ")

        if self.stats['http_errors'] or self.stats['url_errors'] or self.stats['other_errors']:
            print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
            if self.stats['http_errors']:
                print(f"   HTTPé”™è¯¯: {self.stats['http_errors']}")
            if self.stats['url_errors']:
                print(f"   URLé”™è¯¯: {self.stats['url_errors']}")
            if self.stats['other_errors']:
                print(f"   å…¶ä»–é”™è¯¯: {self.stats['other_errors']}")

        print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(self.output_dir)}/")
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸ•·ï¸  æ— ä¾èµ–ç½‘ç»œçˆ¬è™« (ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“)")
    print("=" * 80)

    # è·å–ç”¨æˆ·è¾“å…¥
    print("\nè¯·é€‰æ‹©çˆ¬å–æ¨¡å¼:")
    print("1. çˆ¬å–æœ¬åœ°Narutoç½‘ç«™ (http://localhost:8000)")
    print("2. çˆ¬å–ç¤ºä¾‹ç½‘ç«™ (http://example.com)")
    print("3. è‡ªå®šä¹‰URL")

    choice = input("\nè¯·é€‰æ‹© (1/2/3ï¼Œé»˜è®¤1): ").strip() or "1"

    if choice == "1":
        url = "http://localhost:8000"
        print("\nâš ï¸  è¯·ç¡®ä¿å·²è¿è¡Œ: cd naruto-website && python serve.py")
        input("ç¡®è®¤åæŒ‰å›è½¦ç»§ç»­...")
        download_imgs = True
    elif choice == "2":
        url = "http://example.com"
        download_imgs = False
    else:
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url.startswith('http'):
            url = 'http://' + url
        download_imgs = input("æ˜¯å¦ä¸‹è½½å›¾ç‰‡? (y/nï¼Œé»˜è®¤n): ").strip().lower() == 'y'

    # è·å–çˆ¬å–å‚æ•°
    try:
        max_pages = int(input(f"æœ€å¤§é¡µé¢æ•° (é»˜è®¤10): ").strip() or "10")
    except:
        max_pages = 10

    try:
        delay = float(input(f"è¯·æ±‚å»¶è¿Ÿç§’æ•° (é»˜è®¤1): ").strip() or "1")
    except:
        delay = 1

    # åˆ›å»ºçˆ¬è™«å¹¶å¼€å§‹çˆ¬å–
    crawler = StdlibCrawler(
        base_url=url,
        max_pages=max_pages,
        delay=delay,
        download_images=download_imgs
    )

    try:
        crawler.crawl()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        print(f"å·²çˆ¬å– {crawler.stats['pages_crawled']} ä¸ªé¡µé¢")
        crawler.save_results(0)


if __name__ == "__main__":
    main()
