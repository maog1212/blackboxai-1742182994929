# å°è« OCR - æœ¬åœ°éƒ¨ç½²æŒ‡å—ï¼ˆæ— éœ€APIï¼‰

> å®Œå…¨åŸºäºGitHubå¼€æºä»£ç çš„æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ

## ğŸ“¦ å®˜æ–¹å¼€æºä»“åº“

DeepSeek-OCR æ˜¯å®Œå…¨å¼€æºçš„é¡¹ç›®ï¼Œæ— éœ€ä»»ä½•APIå¯†é’¥ï¼

**å®˜æ–¹GitHub**: https://github.com/deepseek-ai/DeepSeek-OCR
**HuggingFace**: https://huggingface.co/deepseek-ai/DeepSeek-OCR

## ğŸš€ æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨å®˜æ–¹æºä»£ç ï¼ˆæ¨èï¼‰

### 1. å…‹éš†å®˜æ–¹ä»“åº“

```bash
# å…‹éš†DeepSeek-OCRå®˜æ–¹ä»“åº“
git clone https://github.com/deepseek-ai/DeepSeek-OCR.git
cd DeepSeek-OCR
```

### 2. åˆ›å»ºPythonç¯å¢ƒ

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr
```

### 3. å®‰è£…ä¾èµ–

```bash
# å®‰è£…PyTorch (CUDA 11.8)
pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers==4.46.3
pip install tokenizers==0.20.3
pip install einops
pip install addict
pip install easydict
pip install flash-attn==2.7.3
pip install Pillow
pip install vllm==0.8.5
```

### 4. ç›´æ¥ä½¿ç”¨å®˜æ–¹ä»£ç 

#### æ–¹å¼ A: ä½¿ç”¨ Transformersï¼ˆæœ€ç®€å•ï¼‰

åˆ›å»ºæ–‡ä»¶ `simple_ocr.py`:

```python
from transformers import AutoModel, AutoTokenizer
import torch
import os

# è®¾ç½®GPU
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šè‡ªåŠ¨ä»HuggingFaceä¸‹è½½ï¼Œçº¦6.67GBï¼‰
model_name = 'deepseek-ai/DeepSeek-OCR'
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name,
    _attn_implementation='flash_attention_2',  # ä½¿ç”¨Flash AttentionåŠ é€Ÿ
    trust_remote_code=True,
    use_safetensors=True
)

# æ¨¡å‹åŠ è½½åˆ°GPUå¹¶è®¾ç½®ä¸ºåŠç²¾åº¦
model = model.eval().cuda().to(torch.bfloat16)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# OCRè¯†åˆ«
def ocr_image(image_path, output_dir='./outputs', mode='ocr'):
    """
    OCRè¯†åˆ«å‡½æ•°

    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        mode: è¯†åˆ«æ¨¡å¼
            - 'ocr': é€šç”¨OCR
            - 'doc2md': æ–‡æ¡£è½¬Markdown
            - 'grounding': å¸¦åæ ‡çš„OCR
    """

    # æ„å»ºæç¤ºè¯
    if mode == 'doc2md':
        prompt = "<image>\n<|grounding|>Convert the document to markdown."
    elif mode == 'grounding':
        prompt = "<image>\n<|grounding|>Extract text with positions."
    else:
        prompt = "<image>\nExtract all text from the image."

    # æ‰§è¡Œæ¨ç†
    result = model.infer(
        tokenizer,
        prompt=prompt,
        image_file=image_path,
        output_path=output_dir,
        base_size=1024,      # åŸºç¡€åˆ†è¾¨ç‡
        image_size=640,      # å›¾å—å¤§å°
        crop_mode=True,      # å¯ç”¨è£å‰ªæ¨¡å¼
        save_results=True,   # ä¿å­˜ç»“æœ
        test_compress=True   # æµ‹è¯•å‹ç¼©
    )

    return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è¯†åˆ«å•å¼ å›¾ç‰‡
    result = ocr_image('test.jpg', output_dir='./outputs', mode='ocr')
    print("\nè¯†åˆ«ç»“æœ:")
    print(result)
```

è¿è¡Œï¼š

```bash
python simple_ocr.py
```

#### æ–¹å¼ B: ä½¿ç”¨ vLLMï¼ˆé«˜æ€§èƒ½ï¼‰

å®˜æ–¹ä»“åº“å·²ç»æä¾›äº†vLLMç‰ˆæœ¬ï¼Œåœ¨ `DeepSeek-OCR-vllm/` ç›®å½•ä¸‹ï¼š

```bash
cd DeepSeek-OCR-vllm

# ä¿®æ”¹é…ç½®
vim config.py  # è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„

# å¤„ç†å›¾ç‰‡
python run_dpsk_ocr_image.py

# å¤„ç†PDF
python run_dpsk_ocr_pdf.py
```

å®˜æ–¹vLLMä»£ç ç¤ºä¾‹ï¼š

```python
from vllm import LLM, SamplingParams
from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor

# åˆå§‹åŒ–vLLM
llm = LLM(
    model="deepseek-ai/DeepSeek-OCR",
    enable_prefix_caching=False,
    mm_processor_cache_gb=0,
    logits_processors=[NGramPerReqLogitsProcessor],
    trust_remote_code=True
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=8192
)

# å‡†å¤‡è¾“å…¥
inputs = {
    "prompt": "Extract text from this image",
    "multi_modal_data": {"image": "path/to/image.jpg"}
}

# æ‰§è¡Œæ¨ç†ï¼ˆé€Ÿåº¦è¶…å¿«ï¼ï¼‰
outputs = llm.generate(inputs, sampling_params)
print(outputs[0].outputs[0].text)
```

## ğŸ”§ æ–¹æ¡ˆäºŒï¼šä½¿ç”¨Rustç‰ˆæœ¬ï¼ˆæ— éœ€Pythonï¼‰

å¦‚æœä½ ä¸æƒ³é…ç½®Pythonç¯å¢ƒï¼Œè¿˜æœ‰Rustç‰ˆæœ¬ï¼

```bash
# å…‹éš†Rustå®ç°
git clone https://github.com/TimmyOVO/deepseek-ocr.rs.git
cd deepseek-ocr.rs

# ç¼–è¯‘ï¼ˆæˆ–ç›´æ¥ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼‰
cargo build --release

# ç›´æ¥è¿è¡Œ
./target/release/deepseek-ocr --image test.jpg
```

## ğŸ“– å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: æ‰¹é‡å¤„ç†å›¾ç‰‡

```python
import os
from glob import glob
from simple_ocr import ocr_image

# æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡
image_dir = './images'
output_dir = './outputs'

for image_path in glob(f"{image_dir}/*.jpg"):
    print(f"å¤„ç†: {image_path}")
    result = ocr_image(image_path, output_dir, mode='ocr')
    print(f"âœ… å®Œæˆ")
```

### ç¤ºä¾‹2: å¤„ç†PDFæ–‡æ¡£

```python
from pdf2image import convert_from_path
from simple_ocr import ocr_image
import os

def ocr_pdf(pdf_path, output_dir):
    """å¤„ç†PDFæ–‡ä»¶"""

    # PDFè½¬å›¾ç‰‡
    print(f"è½¬æ¢PDF: {pdf_path}")
    images = convert_from_path(pdf_path)

    all_text = []

    # é€é¡µå¤„ç†
    for i, image in enumerate(images):
        print(f"å¤„ç†ç¬¬ {i+1}/{len(images)} é¡µ...")

        # ä¿å­˜ä¸´æ—¶å›¾ç‰‡
        temp_path = f"{output_dir}/temp_page_{i}.jpg"
        image.save(temp_path, 'JPEG')

        # OCRè¯†åˆ«
        result = ocr_image(temp_path, output_dir, mode='doc2md')
        all_text.append(f"\n--- ç¬¬ {i+1} é¡µ ---\n{result}")

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)

    # ä¿å­˜å®Œæ•´ç»“æœ
    output_file = f"{output_dir}/full_result.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_text))

    print(f"âœ… PDFå¤„ç†å®Œæˆ: {output_file}")
    return output_file

# ä½¿ç”¨
ocr_pdf('document.pdf', './outputs')
```

### ç¤ºä¾‹3: Webç•Œé¢è°ƒç”¨æœ¬åœ°æ¨¡å‹

ä¿®æ”¹æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ `backend/deepseek_ocr.py`ï¼Œç›´æ¥è°ƒç”¨æœ¬åœ°æ¨¡å‹ï¼š

```python
from transformers import AutoModel, AutoTokenizer
import torch

class LocalDeepSeekOCR:
    """æœ¬åœ°DeepSeek-OCRï¼ˆæ— APIï¼‰"""

    def __init__(self):
        self.model = None
        self.tokenizer = None

    def load_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½DeepSeek-OCRæ¨¡å‹...")

        model_name = 'deepseek-ai/DeepSeek-OCR'

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )

        self.model = AutoModel.from_pretrained(
            model_name,
            _attn_implementation='flash_attention_2',
            trust_remote_code=True,
            use_safetensors=True
        )

        self.model = self.model.eval().cuda().to(torch.bfloat16)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    def recognize(self, image_path, mode='ocr'):
        """è¯†åˆ«å›¾ç‰‡"""
        if mode == 'doc2md':
            prompt = "<image>\n<|grounding|>Convert the document to markdown."
        else:
            prompt = "<image>\nExtract all text."

        result = self.model.infer(
            self.tokenizer,
            prompt=prompt,
            image_file=image_path,
            base_size=1024,
            image_size=640,
            crop_mode=True
        )

        return result

# åœ¨Flask APIä¸­ä½¿ç”¨
ocr = LocalDeepSeekOCR()
ocr.load_model()
```

## âš™ï¸ é…ç½®è¯´æ˜

### åˆ†è¾¨ç‡è®¾ç½®

```python
# ä¸åŒåˆ†è¾¨ç‡å¯¹åº”çš„tokenæ•°é‡
resolutions = {
    '512x512': 64,      # æœ€å¿«
    '768x768': 144,     # å¿«é€Ÿ
    '1024x1024': 256,   # æ¨è
    '1280x1280': 400    # æœ€ç²¾ç¡®
}

# åœ¨inferä¸­è®¾ç½®
result = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_path,
    base_size=1024,  # ä¿®æ”¹è¿™é‡Œ
    image_size=640
)
```

### GPUæ˜¾å­˜ä¼˜åŒ–

```python
# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼ˆ<12GBï¼‰
model = model.eval().cuda().to(torch.float16)  # ä½¿ç”¨float16

# æˆ–è€…ä½¿ç”¨CPUï¼ˆæ…¢ä½†ç¨³å®šï¼‰
model = model.eval().cpu().to(torch.float32)
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | ç¡¬ä»¶è¦æ±‚ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|----------|
| **vLLM + A100** | 40GBæ˜¾å­˜ | âš¡âš¡âš¡âš¡âš¡ | ç”Ÿäº§ç¯å¢ƒï¼Œå¤§æ‰¹é‡ |
| **Transformers + RTX 3060** | 12GBæ˜¾å­˜ | âš¡âš¡âš¡ | ä¸ªäººä½¿ç”¨ï¼Œä¸­ç­‰æ‰¹é‡ |
| **CPUæ¨¡å¼** | 16GBå†…å­˜ | âš¡ | æµ‹è¯•ï¼Œå°æ‰¹é‡ |
| **Rustç‰ˆæœ¬** | ä»»æ„ | âš¡âš¡âš¡âš¡ | å‘½ä»¤è¡Œå·¥å…· |

## ğŸ¯ ä¼˜åŠ¿æ€»ç»“

âœ… **å®Œå…¨å¼€æº** - æ— éœ€ä»»ä½•APIå¯†é’¥
âœ… **æœ¬åœ°è¿è¡Œ** - æ•°æ®éšç§100%ä¿æŠ¤
âœ… **ç¦»çº¿å¯ç”¨** - æ¨¡å‹ä¸‹è½½åå¯ç¦»çº¿ä½¿ç”¨
âœ… **å…è´¹ä½¿ç”¨** - æ— ä»»ä½•è´¹ç”¨
âœ… **é«˜æ€§èƒ½** - å•GPUæ—¥å¤„ç†20ä¸‡é¡µ
âœ… **å¤šç§æ–¹æ¡ˆ** - Python/Rust/vLLM ä»»é€‰

## ğŸ“ å¸¸è§é—®é¢˜

**Q: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹å—ï¼Ÿ**
A: æ˜¯çš„ï¼Œé¦–æ¬¡ä¼šè‡ªåŠ¨ä»HuggingFaceä¸‹è½½çº¦6.67GBçš„æ¨¡å‹æ–‡ä»¶ã€‚ä¸‹è½½åä¼šç¼“å­˜åˆ°æœ¬åœ°ã€‚

**Q: å¯ä»¥å®Œå…¨ç¦»çº¿ä½¿ç”¨å—ï¼Ÿ**
A: å¯ä»¥ï¼æ¨¡å‹ä¸‹è½½åï¼Œå¯ä»¥å®Œå…¨ç¦»çº¿è¿è¡Œã€‚

**Q: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ**
A: å¯ä»¥ä½¿ç”¨CPUæ¨¡å¼ï¼Œæˆ–è€…é™ä½åˆ†è¾¨ç‡ï¼Œæˆ–è€…ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ã€‚

**Q: å¦‚ä½•åŠ é€Ÿä¸‹è½½ï¼Ÿ**
A: ä½¿ç”¨HuggingFaceé•œåƒï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹ä»“åº“**: https://github.com/deepseek-ai/DeepSeek-OCR
- **Rustç‰ˆæœ¬**: https://github.com/TimmyOVO/deepseek-ocr.rs
- **æ¨¡å‹ä¸‹è½½**: https://huggingface.co/deepseek-ai/DeepSeek-OCR
- **ä½¿ç”¨æ•™ç¨‹**: https://blog.csdn.net/qq_58607032/article/details/153774639

---

**å®Œå…¨å¼€æºï¼Œæ— éœ€APIï¼Œæœ¬åœ°è¿è¡Œï¼Œæ•°æ®å®‰å…¨ï¼** ğŸš€
