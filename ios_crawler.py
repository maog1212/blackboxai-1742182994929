#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
iOS ä¼˜åŒ–ç‰ˆç½‘ç»œçˆ¬è™«
ä¸“ä¸º iPhone/iPad çš„ Pythonistaã€a-Shell ç­‰åº”ç”¨è®¾è®¡
é›¶ä¾èµ– - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“
"""

import urllib.request
import urllib.parse
from html.parser import HTMLParser
import json
import sys
from datetime import datetime


class SimpleHTMLParser(HTMLParser):
    """ç®€åŒ–çš„HTMLè§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.title = ""
        self.links = []
        self.images = []
        self.headings = []
        self._in_title = False
        self._current_heading = None

    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)

        if tag == 'title':
            self._in_title = True
        elif tag == 'a' and 'href' in attrs_dict:
            self.links.append(attrs_dict['href'])
        elif tag == 'img' and 'src' in attrs_dict:
            self.images.append(attrs_dict['src'])
        elif tag in ['h1', 'h2', 'h3']:
            self._current_heading = tag

    def handle_endtag(self, tag):
        if tag == 'title':
            self._in_title = False
        elif tag in ['h1', 'h2', 'h3']:
            self._current_heading = None

    def handle_data(self, data):
        if self._in_title:
            self.title += data.strip()
        elif self._current_heading:
            text = data.strip()
            if text:
                self.headings.append(text)


class IOSCrawler:
    """iOSä¼˜åŒ–çš„çˆ¬è™«ç±»"""

    def __init__(self):
        self.results = []

    def fetch(self, url):
        """è·å–ç½‘é¡µ"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X)'
        }
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=8) as response:
                data = response.read()
                try:
                    return data.decode('utf-8')
                except:
                    return data.decode('utf-8', errors='ignore')
        except Exception as e:
            return None

    def parse(self, html, base_url):
        """è§£æHTML"""
        parser = SimpleHTMLParser()
        try:
            parser.feed(html)
        except:
            pass

        # å¤„ç†é“¾æ¥
        links = []
        for link in parser.links[:15]:
            try:
                full_url = urllib.parse.urljoin(base_url, link)
                if full_url.startswith('http'):
                    links.append(full_url)
            except:
                pass

        return {
            'title': parser.title or 'æ— æ ‡é¢˜',
            'links': links,
            'images': len(parser.images),
            'headings': parser.headings[:10]
        }

    def crawl(self, url):
        """çˆ¬å–å•ä¸ªURL"""
        if not url.startswith('http'):
            url = 'http://' + url

        print(f"\n{'='*50}")
        print(f"ğŸ“± æ­£åœ¨çˆ¬å–: {url}")
        print(f"{'='*50}\n")

        html = self.fetch(url)

        if not html:
            print("âŒ è·å–å¤±è´¥\n")
            return None

        data = self.parse(html, url)

        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… æ ‡é¢˜: {data['title']}")
        print(f"ğŸ”— é“¾æ¥æ•°: {len(data['links'])}")
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æ•°: {data['images']}")

        if data['headings']:
            print(f"\nğŸ“‹ ä¸»è¦æ ‡é¢˜:")
            for i, h in enumerate(data['headings'][:5], 1):
                print(f"  {i}. {h}")

        if data['links']:
            print(f"\nğŸ”— å‘ç°çš„é“¾æ¥:")
            for i, link in enumerate(data['links'][:10], 1):
                # ç¼©çŸ­æ˜¾ç¤º
                display = link if len(link) < 50 else link[:47] + '...'
                print(f"  {i}. {display}")

        print(f"\n{'='*50}\n")

        result = {
            'url': url,
            'time': datetime.now().isoformat(),
            'data': data
        }

        self.results.append(result)
        return result

    def save_json(self, filename='crawler_results.json'):
        """ä¿å­˜ä¸ºJSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")


def interactive_mode():
    """äº¤äº’æ¨¡å¼ - é€‚åˆæ‰‹æœºä½¿ç”¨"""
    crawler = IOSCrawler()

    print("\n" + "="*50)
    print("ğŸ“± iOS ç½‘ç»œçˆ¬è™«")
    print("="*50)
    print("\næç¤º: è¾“å…¥ 'q' é€€å‡º, 's' ä¿å­˜ç»“æœ\n")

    while True:
        try:
            url = input("ğŸ” è¾“å…¥URL (æˆ–å‘½ä»¤): ").strip()

            if not url:
                continue

            if url.lower() == 'q':
                print("\nğŸ‘‹ å†è§!")
                break

            if url.lower() == 's':
                if crawler.results:
                    crawler.save_json()
                else:
                    print("âš ï¸  æš‚æ— ç»“æœå¯ä¿å­˜")
                continue

            if url.lower() == 'h' or url == '?':
                print("\nğŸ“– å¸®åŠ©:")
                print("  - ç›´æ¥è¾“å…¥URLå¼€å§‹çˆ¬å–")
                print("  - 's' = ä¿å­˜ç»“æœåˆ°JSON")
                print("  - 'q' = é€€å‡ºç¨‹åº")
                print("  - 'demo' = è¿è¡Œæ¼”ç¤º\n")
                continue

            if url.lower() == 'demo':
                demo_urls = [
                    'http://example.com',
                    'http://localhost:8000'
                ]
                print("\né€‰æ‹©æ¼”ç¤ºURL:")
                for i, u in enumerate(demo_urls, 1):
                    print(f"  {i}. {u}")

                choice = input("\né€‰æ‹© (1-2): ").strip()
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(demo_urls):
                        url = demo_urls[idx]
                    else:
                        continue
                except:
                    continue

            # çˆ¬å–
            crawler.crawl(url)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")

    # é€€å‡ºæ—¶è¯¢é—®æ˜¯å¦ä¿å­˜
    if crawler.results:
        save = input("\nğŸ’¾ ä¿å­˜ç»“æœ? (y/n): ").strip().lower()
        if save == 'y':
            crawler.save_json()


def quick_crawl(url):
    """å¿«é€Ÿçˆ¬å–å•ä¸ªURL"""
    crawler = IOSCrawler()
    result = crawler.crawl(url)

    if result:
        save = input("\nğŸ’¾ ä¿å­˜ç»“æœ? (y/n): ").strip().lower()
        if save == 'y':
            crawler.save_json()


def batch_crawl(urls):
    """æ‰¹é‡çˆ¬å–"""
    crawler = IOSCrawler()

    print(f"\nğŸ“‹ æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªURL\n")

    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}]")
        crawler.crawl(url)

    crawler.save_json()
    print(f"\nâœ… å®Œæˆ! å…±çˆ¬å– {len(crawler.results)} ä¸ªé¡µé¢")


def main():
    """ä¸»å‡½æ•°"""

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        url = sys.argv[1]

        # æ‰¹é‡æ¨¡å¼
        if len(sys.argv) > 2:
            urls = sys.argv[1:]
            batch_crawl(urls)
        else:
            quick_crawl(url)
    else:
        # äº¤äº’æ¨¡å¼
        interactive_mode()


# ä¸ºPythonistaç­‰åº”ç”¨æä¾›çš„å¿«æ·å‡½æ•°
def crawl(url):
    """
    å¿«æ·å‡½æ•° - çˆ¬å–å•ä¸ªURL

    ç”¨æ³•:
        import ios_crawler
        ios_crawler.crawl('http://example.com')
    """
    quick_crawl(url)


def crawl_multiple(*urls):
    """
    å¿«æ·å‡½æ•° - çˆ¬å–å¤šä¸ªURL

    ç”¨æ³•:
        import ios_crawler
        ios_crawler.crawl_multiple('url1', 'url2', 'url3')
    """
    batch_crawl(list(urls))


if __name__ == "__main__":
    main()
