#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„ç½‘ç»œçˆ¬è™«ç¤ºä¾‹
A Simple Web Crawler Example
"""

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
import os


class SimpleCrawler:
    """ç®€å•çš„ç½‘ç»œçˆ¬è™«ç±»"""

    def __init__(self, base_url, max_pages=10, delay=1):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            base_url: èµ·å§‹URL
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œéµå®ˆçˆ¬è™«ç¤¼ä»ª
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited_urls = set()
        self.to_visit = [base_url]

        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def is_valid_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å±äºåŒä¸€åŸŸå"""
        parsed_base = urlparse(self.base_url)
        parsed_url = urlparse(url)

        # åªçˆ¬å–åŒä¸€åŸŸåä¸‹çš„é¡µé¢
        return parsed_url.netloc == parsed_base.netloc

    def get_page_content(self, url):
        """
        è·å–é¡µé¢å†…å®¹

        Args:
            url: è¦çˆ¬å–çš„URL

        Returns:
            BeautifulSoupå¯¹è±¡ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            return BeautifulSoup(response.text, 'html.parser')

        except requests.RequestException as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_links(self, soup, current_url):
        """
        ä»é¡µé¢ä¸­æå–æ‰€æœ‰é“¾æ¥

        Args:
            soup: BeautifulSoupå¯¹è±¡
            current_url: å½“å‰é¡µé¢URL

        Returns:
            é“¾æ¥åˆ—è¡¨
        """
        links = []

        for link in soup.find_all('a', href=True):
            href = link['href']
            # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
            absolute_url = urljoin(current_url, href)

            # åªæ·»åŠ æœ‰æ•ˆçš„URL
            if self.is_valid_url(absolute_url):
                links.append(absolute_url)

        return links

    def extract_data(self, soup, url):
        """
        ä»é¡µé¢ä¸­æå–æ•°æ®

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: é¡µé¢URL

        Returns:
            æå–çš„æ•°æ®å­—å…¸
        """
        # æå–æ ‡é¢˜
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"

        # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬
        paragraphs = [p.get_text().strip() for p in soup.find_all('p')]

        # æå–æ‰€æœ‰å›¾ç‰‡
        images = [img.get('src', '') for img in soup.find_all('img')]

        data = {
            'url': url,
            'title': title_text,
            'paragraphs_count': len(paragraphs),
            'images_count': len(images),
            'images': images[:5]  # åªæ˜¾ç¤ºå‰5å¼ å›¾ç‰‡
        }

        return data

    def save_to_file(self, data, filename='crawler_results.txt'):
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°æ–‡ä»¶"""
        with open(filename, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*80}\n")
            f.write(f"URL: {data['url']}\n")
            f.write(f"æ ‡é¢˜: {data['title']}\n")
            f.write(f"æ®µè½æ•°: {data['paragraphs_count']}\n")
            f.write(f"å›¾ç‰‡æ•°: {data['images_count']}\n")
            if data['images']:
                f.write(f"å›¾ç‰‡åˆ—è¡¨: {', '.join(data['images'])}\n")

    def crawl(self):
        """å¼€å§‹çˆ¬å–"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {self.base_url}")
        print(f"ğŸ“ æœ€å¤§é¡µé¢æ•°: {self.max_pages}")
        print(f"â±ï¸  è¯·æ±‚å»¶è¿Ÿ: {self.delay}ç§’\n")

        # å¦‚æœç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists('crawler_results.txt'):
            os.remove('crawler_results.txt')

        page_count = 0

        while self.to_visit and page_count < self.max_pages:
            # è·å–ä¸‹ä¸€ä¸ªè¦è®¿é—®çš„URL
            current_url = self.to_visit.pop(0)

            # è·³è¿‡å·²è®¿é—®çš„URL
            if current_url in self.visited_urls:
                continue

            print(f"ğŸ“„ æ­£åœ¨çˆ¬å– [{page_count + 1}/{self.max_pages}]: {current_url}")

            # æ ‡è®°ä¸ºå·²è®¿é—®
            self.visited_urls.add(current_url)

            # è·å–é¡µé¢å†…å®¹
            soup = self.get_page_content(current_url)

            if soup:
                # æå–æ•°æ®
                data = self.extract_data(soup, current_url)
                print(f"   âœ… æ ‡é¢˜: {data['title']}")
                print(f"   ğŸ“Š æ®µè½: {data['paragraphs_count']}, å›¾ç‰‡: {data['images_count']}")

                # ä¿å­˜æ•°æ®
                self.save_to_file(data)

                # æå–æ–°çš„é“¾æ¥
                new_links = self.extract_links(soup, current_url)

                # æ·»åŠ æœªè®¿é—®çš„é“¾æ¥åˆ°å¾…è®¿é—®åˆ—è¡¨
                for link in new_links:
                    if link not in self.visited_urls and link not in self.to_visit:
                        self.to_visit.append(link)

                page_count += 1

            # å»¶è¿Ÿï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
            if page_count < self.max_pages:
                time.sleep(self.delay)

        print(f"\nâœ¨ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ˆ æ€»å…±çˆ¬å–äº† {page_count} ä¸ªé¡µé¢")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° crawler_results.txt")


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""

    # ç¤ºä¾‹1: çˆ¬å–æœ¬åœ°Narutoç½‘ç«™ï¼ˆéœ€è¦å…ˆè¿è¡Œserve.pyï¼‰
    print("=" * 80)
    print("ç®€å•ç½‘ç»œçˆ¬è™«ç¤ºä¾‹")
    print("=" * 80)
    print("\nè¯·é€‰æ‹©çˆ¬å–æ¨¡å¼:")
    print("1. çˆ¬å–æœ¬åœ°Narutoç½‘ç«™ (http://localhost:8000)")
    print("2. çˆ¬å–å…¶ä»–ç½‘ç«™ (è¾“å…¥URL)")
    print("3. ä½¿ç”¨é»˜è®¤ç¤ºä¾‹ (example.com)")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

    if choice == "1":
        url = "http://localhost:8000"
        print("\nâš ï¸  è¯·ç¡®ä¿å·²ç»è¿è¡Œäº† serve.py (python naruto-website/serve.py)")
        input("æŒ‰å›è½¦é”®ç»§ç»­...")
    elif choice == "2":
        url = input("è¯·è¾“å…¥è¦çˆ¬å–çš„URL: ").strip()
        if not url.startswith('http'):
            url = 'http://' + url
    else:
        url = "http://example.com"

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = SimpleCrawler(
        base_url=url,
        max_pages=10,      # æœ€å¤šçˆ¬å–10ä¸ªé¡µé¢
        delay=1            # æ¯æ¬¡è¯·æ±‚é—´éš”1ç§’
    )

    # å¼€å§‹çˆ¬å–
    try:
        crawler.crawl()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        print(f"å·²çˆ¬å– {len(crawler.visited_urls)} ä¸ªé¡µé¢")


if __name__ == "__main__":
    main()
